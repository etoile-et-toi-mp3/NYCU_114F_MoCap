{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Read bag from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import library\n",
    "import pyrealsense2 as rs\n",
    "# Import Numpy for easy array manipulation\n",
    "import numpy as np\n",
    "# Import OpenCV for easy image rendering\n",
    "import cv2\n",
    "# Import argparse for command-line options\n",
    "import argparse\n",
    "# Import os.path for file path manipulation\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object for parsing command-line options\n",
    "PFX = '../../../bag_files/'\n",
    "parser = argparse.ArgumentParser(description=\"Read recorded bag file and display depth stream in jet colormap.\\\n",
    "                                Remember to change the stream fps and format to match the recorded.\")\n",
    "# Add argument which takes path to a bag file as an input\n",
    "parser.add_argument(\"-i\", \"--input\", default=PFX + '001.bag',type=str, help=\"Path to the bag file\")\n",
    "# Parse the command line arguments to an object\n",
    "args = parser.parse_args(args=['--input', PFX + '001.bag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety if no parameter have been given\n",
    "if not args.input:\n",
    "    print(\"No input paramater have been given.\")\n",
    "    print(\"For help type --help\")\n",
    "    exit()\n",
    "# Check if the given file have bag extension\n",
    "if os.path.splitext(args.input)[1] != \".bag\":\n",
    "    print(\"The given file is not of correct file format.\")\n",
    "    print(\"Only .bag files are accepted\")\n",
    "    exit()\n",
    "try:\n",
    "    # Create pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "\n",
    "    # Create a config object\n",
    "    config = rs.config()\n",
    "\n",
    "    # Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "    rs.config.enable_device_from_file(config, args.input)\n",
    "\n",
    "    # Configure the pipeline to stream the depth stream\n",
    "    # Change this parameters according to the recorded bag file resolution\n",
    "    # config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "\n",
    "    # Start streaming from file\n",
    "    pipeline.start(config)\n",
    "\n",
    "    # Create opencv window to render image in\n",
    "    cv2.namedWindow(\"Depth Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"Color Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    # Create colorizer object\n",
    "    colorizer = rs.colorizer()\n",
    "\n",
    "    # Streaming loop\n",
    "    while True:\n",
    "        # Get frameset of depth\n",
    "        frames = pipeline.wait_for_frames()\n",
    "\n",
    "        # Get depth frame\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        # Colorize depth frame to jet colormap\n",
    "        depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "        # Convert depth_frame to numpy array to render image in opencv\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "        # Render image in opencv window\n",
    "        cv2.imshow(\"Depth Stream\", depth_color_image)\n",
    "        cv2.imshow(\"Color Stream\", color_image)\n",
    "        key = cv2.waitKey(1)\n",
    "        # if pressed escape exit program\n",
    "        if key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork (Teamwork) : Mediapipe from a bag\n",
    "Please write a code to track the movements of a person from a bag file. For a instance:\n",
    "1. track Facial Landmarks \n",
    "2. track Hand Poses\n",
    "3. track Body Poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to resolve request. Request to enable_device_from_file(\"001.bag\") was invalid, Reason: Failed to create ros reader: Error opening file: 001.bag",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     18\u001b[0m rs\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_device_from_file(config, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m001.bag\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Configure the pipeline to stream the depth stream\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Change this parameters according to the recorded bag file resolution\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#config.enable_stream(rs.stream.depth, rs.format.z16, 30)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Start streaming from file\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Initiate holistic model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_holistic\u001b[38;5;241m.\u001b[39mHolistic(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m holistic:\n\u001b[0;32m     31\u001b[0m     \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     while cap.isOpened():\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to resolve request. Request to enable_device_from_file(\"001.bag\") was invalid, Reason: Failed to create ros reader: Error opening file: 001.bag"
     ]
    }
   ],
   "source": [
    "# First import library\n",
    "import pyrealsense2 as rs\n",
    "import mediapipe as mp      \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# Create pipeline\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "# Create a config object\n",
    "config = rs.config()\n",
    "\n",
    "# Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "rs.config.enable_device_from_file(config, '001.bag')\n",
    "\n",
    "# Configure the pipeline to stream the depth stream\n",
    "# Change this parameters according to the recorded bag file resolution\n",
    "#config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "#config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "\n",
    "# Start streaming from file\n",
    "pipeline.start(config)\n",
    "\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "#     while cap.isOpened():\n",
    "    while True:\n",
    "\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        frame = frames.get_color_frame()\n",
    "        color_image = np.asanyarray(frame.get_data())\n",
    "\n",
    "\n",
    "#         ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Draw face landmarks\n",
    "        #mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION)\n",
    "        \n",
    "        # Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                        \n",
    "        cv2.imshow('mediapipe', image)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27: #press ESC key to quit\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Read Color and Depth from the Intel RealSense camera\n",
    "\n",
    "Please run on Jupyter Notebook with root on MacBook enviroments\n",
    "\n",
    "**sudo jupyter notebook --allow-root**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## License: Apache 2.0. See LICENSE file in root directory.\n",
    "## Copyright(c) 2015-2017 Intel Corporation. All Rights Reserved.\n",
    "\n",
    "###############################################\n",
    "##      Open CV and Numpy integration        ##\n",
    "###############################################\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "#config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "\n",
    "#config.enable_record_to_file('record.bag')\n",
    "\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "        color_colormap_dim = color_image.shape\n",
    "\n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "            images = np.hstack((resized_color_image, depth_colormap))\n",
    "        else:\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27: #press ESC key to quit\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "finally:\n",
    "        pipeline.stop()            # Stop streaming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Background subtraction using depth information\n",
    "![title](background_removal.png)\n",
    "\n",
    "source: https://miro.medium.com/max/1400/1*QPmjt_wQiEBEheUler07zw.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import the library\n",
    "import pyrealsense2 as rs\n",
    "# Import Numpy for easy array manipulation\n",
    "import numpy as np\n",
    "# Import OpenCV for easy image rendering\n",
    "import cv2\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "# Create a config object\n",
    "config = rs.config()\n",
    "\n",
    "# Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "rs.config.enable_device_from_file(config, '001.bag')\n",
    "\n",
    "# Configure the pipeline to stream the depth stream\n",
    "# Change this parameters according to the recorded bag file resolution\n",
    "# config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "\n",
    "# Start streaming\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "# Getting the depth sensor's depth scale (see rs-align example for explanation)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "print(\"Depth Scale is: \" , depth_scale)\n",
    "\n",
    "# We will be removing the background of objects more than\n",
    "#  clipping_distance_in_meters meters away\n",
    "clipping_distance_in_meters = 1 #1 meter\n",
    "clipping_distance = clipping_distance_in_meters / depth_scale\n",
    "\n",
    "# Create an align object\n",
    "# rs.align allows us to perform alignment of depth frames to others frames\n",
    "# The \"align_to\" is the stream type to which we plan to align depth frames.\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "# Streaming loop\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        # frames.get_depth_frame() is a 640x360 depth image\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame() # aligned_depth_frame is a 640x480 depth image\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        depth_image = np.asanyarray(aligned_depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Remove background - Set pixels further than clipping_distance to grey\n",
    "        grey_color = 153\n",
    "        depth_image_3d = np.dstack((depth_image,depth_image,depth_image)) #depth image is 1 channel, color is 3 channels\n",
    "        bg_removed = np.where((depth_image_3d > clipping_distance) | (depth_image_3d <= 0), grey_color, color_image)\n",
    "\n",
    "        # Render images:\n",
    "        #   depth align to color on left\n",
    "        #   depth on right\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        images = np.hstack((bg_removed, depth_colormap))\n",
    "\n",
    "        cv2.namedWindow('Align Example', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('Align Example', images)\n",
    "        key = cv2.waitKey(1)\n",
    "        # Press esc or 'q' to close the image window\n",
    "        if key & 0xFF == ord('q') or key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "finally:\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MediaPipe Selfie Segmentation\n",
    "\n",
    "MediaPipe Selfie Segmentation segments the important humans in the scene. It can run in real-time on both smartphones and laptops. The use cases include selfie effects and video conferencing, where the person is close (< 2m) to the camera.\n",
    "\n",
    "\n",
    "\n",
    "![title](mediapipe_selfie.png)\n",
    "\n",
    "source: https://google.github.io/mediapipe/solutions/selfie_segmentation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_selfie_segmentation = mp.solutions.selfie_segmentation\n",
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = []\n",
    "BG_COLOR = (192, 192, 192) # gray\n",
    "MASK_COLOR = (255, 255, 255) # white\n",
    "with mp_selfie_segmentation.SelfieSegmentation(\n",
    "    model_selection=0) as selfie_segmentation:\n",
    "  for idx, file in enumerate(IMAGE_FILES):\n",
    "    image = cv2.imread(file)\n",
    "    image_height, image_width, _ = image.shape\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = selfie_segmentation.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Draw selfie segmentation on the background image.\n",
    "    # To improve segmentation around boundaries, consider applying a joint\n",
    "    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "    # Generate solid color images for showing the output selfie segmentation mask.\n",
    "    fg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "    fg_image[:] = MASK_COLOR\n",
    "    bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "    bg_image[:] = BG_COLOR\n",
    "    output_image = np.where(condition, fg_image, bg_image)\n",
    "    cv2.imwrite('/tmp/selfie_segmentation_output' + str(idx) + '.png', output_image)\n",
    "\n",
    "# For webcam input:\n",
    "BG_COLOR = (192, 192, 192) # gray\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_selfie_segmentation.SelfieSegmentation(\n",
    "    model_selection=1) as selfie_segmentation:\n",
    "  bg_image = None\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading a video, use 'break' instead of 'continue'.\n",
    "      continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\n",
    "    # the BGR image to RGB.\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = selfie_segmentation.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw selfie segmentation on the background image.\n",
    "    # To improve segmentation around boundaries, consider applying a joint\n",
    "    # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "    condition = np.stack(\n",
    "      (results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "    # The background can be customized.\n",
    "    #   a) Load an image (with the same width and height of the input image) to\n",
    "    #      be the qqbackground, e.g., bg_image = cv2.imread('/path/to/image/file')\n",
    "    #   b) Blur the input image by applying image filtering, e.g.,\n",
    "    #      bg_image = cv2.GaussianBlur(image,(55,55),0)\n",
    "    if bg_image is None:\n",
    "      bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "      bg_image[:] = BG_COLOR\n",
    "    output_image = np.where(condition, image, bg_image)\n",
    "\n",
    "    cv2.imshow('MediaPipe Selfie Segmentation', output_image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "      break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork(Teamwork):  Person Detection and Distance Estimation using a RGBD camera\n",
    "\n",
    "Now we have the color and depth information from a RGBD camera. However, how to do a perfact Person Detection and Distance Estimation for real enviroments?\n",
    "\n",
    "\n",
    "![title](PersonDetection.png)\n",
    "https://debuggercafe.com/opencv-hog-for-accurate-and-fast-person-detection/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Person Detection\n",
    "\n",
    "We learn the Person Detection using the COLOR information in the RGB-D camera.\n",
    "In this tutorial, we will learn how to detect persons using the HOG features for person detection.\n",
    "\n",
    "This example is an implementation of a person detector model from OPENCV module.\n",
    "\n",
    "The OPENCV module name is \n",
    "***HOGDescriptor_getDefaultPeopleDetector()***\n",
    "It is trained using Linear SVM machine learning classifier.\n",
    "\n",
    "\n",
    "![title](OpenCV-HOG.PNG)\n",
    "\n",
    "More details\n",
    "\n",
    "https://debuggercafe.com/opencv-hog-for-accurate-and-fast-person-detection/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import library\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "try:\n",
    "    # Create pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "\n",
    "    # Create a config object\n",
    "    config = rs.config()\n",
    "\n",
    "    # Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "    rs.config.enable_device_from_file(config, '002.bag')\n",
    "\n",
    "    # Configure the pipeline to stream the depth stream\n",
    "    # Change this parameters according to the recorded bag file resolution\n",
    "   # config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "\n",
    "    # Start streaming from file\n",
    "    pipeline.start(config)\n",
    "\n",
    "    # Create opencv window to render image in\n",
    "    #cv2.namedWindow(\"Depth Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"Color Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    # Create colorizer object\n",
    "    colorizer = rs.colorizer()\n",
    "    \n",
    "    #People detection\n",
    "    # initialize the HOG descriptor/person detector\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "    # Streaming loop\n",
    "    while True:\n",
    "        # Get frameset of depth\n",
    "        frames = pipeline.wait_for_frames()\n",
    "\n",
    "        # Get depth frame\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        # Colorize depth frame to jet colormap\n",
    "        depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "        # Convert depth_frame to numpy array to render image in opencv\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        \n",
    "        #People detection\n",
    "        \n",
    "        # resizing for faster detection\n",
    "        color_image = cv2.resize(color_image, (640, 360))\n",
    "    \n",
    "        # using a greyscale picture, also for faster detection\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # detect people in the image\n",
    "        # returns the bounding boxes for the detected objects\n",
    "        boxes, weights = hog.detectMultiScale(gray, hitThreshold=0.8, winStride=(8,8) )\n",
    "        \n",
    "        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "        for (xA, yA, xB, yB) in boxes:\n",
    "            # display the detected boxes in the colour picture\n",
    "            cv2.rectangle(color_image, (xA, yA), (xB, yB), (255, 255, 255), 2)\n",
    "    \n",
    "        # Render image in opencv window\n",
    "        #cv2.imshow(\"Depth Stream\", depth_color_image)\n",
    "        cv2.imshow(\"Color Stream\", color_image)\n",
    "        key = cv2.waitKey(1)\n",
    "        # if pressed escape exit program\n",
    "        if key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Distance Estimation \n",
    "We learn the Distance Estimation using the Depth information in the RGB-D camera.\n",
    "In this tutorial, we will learn how to get average distance of a Region Interest Area (ROI) using NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import library\n",
    "import pyrealsense2 as rs\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "\n",
    "    # Create a config object\n",
    "    config = rs.config()\n",
    "\n",
    "    # Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "    rs.config.enable_device_from_file(config, '002.bag')\n",
    "\n",
    "    # Configure the pipeline to stream the depth stream\n",
    "    # Change this parameters according to the recorded bag file resolution\n",
    "   # config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "\n",
    "    # Start streaming from file\n",
    "    pipeline.start(config)\n",
    "\n",
    "    # Create opencv window to render image in\n",
    "    #cv2.namedWindow(\"Depth Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"Color Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    # Create colorizer object\n",
    "    colorizer = rs.colorizer()\n",
    "    \n",
    "    #People detection\n",
    "    # initialize the HOG descriptor/person detector\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    offset=20\n",
    "\n",
    "    # Streaming loop\n",
    "    while True:\n",
    "        # Get frameset of depth\n",
    "        frames = pipeline.wait_for_frames()\n",
    "\n",
    "        # Get depth frame\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        # Colorize depth frame to jet colormap\n",
    "        depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "        # Convert depth_frame to numpy array to render image in opencv\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        \n",
    "        #People detection\n",
    "        \n",
    "        # resizing for faster detection\n",
    "        color_image = cv2.resize(color_image, (640, 360))\n",
    "    \n",
    "        # using a greyscale picture, also for faster detection\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # detect people in the image\n",
    "        # returns the bounding boxes for the detected objects\n",
    "        boxes, weights = hog.detectMultiScale(gray, hitThreshold=0.8, winStride=(8,8) )\n",
    "        \n",
    "        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "        for (xA, yA, xB, yB) in boxes:\n",
    "            # display the detected boxes in the colour picture\n",
    "            cv2.rectangle(color_image, (xA+offset, yA+offset), (xB-offset, yB-offset), (255, 255, 255), 2)\n",
    "            roi_depth = (xA, yA, xB, yB)\n",
    "            mean_depth = np.mean(roi_depth)\n",
    "            distace =str(mean_depth)+' cm'\n",
    "            cv2.putText(color_image, distace, (xA+offset, yA+offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "        # Render image in opencv window\n",
    "        #cv2.imshow(\"Depth Stream\", depth_color_image)\n",
    "        cv2.imshow(\"Color Stream\", color_image)\n",
    "        key = cv2.waitKey(1)\n",
    "        # if pressed escape exit program\n",
    "        if key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write to AVI\n",
    "Next, we prepare the Video Setups and write the results to a video file.\n",
    "Here, there is a need to define the codec for the **VideoWriter**. This will define the output format of the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import library\n",
    "import pyrealsense2 as rs\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "        \n",
    "try:\n",
    "    # Create pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "\n",
    "    # Create a config object\n",
    "    config = rs.config()\n",
    "\n",
    "    # Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "    rs.config.enable_device_from_file(config, '002.bag')\n",
    "\n",
    "    # Configure the pipeline to stream the depth stream\n",
    "    # Change this parameters according to the recorded bag file resolution\n",
    "   # config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "\n",
    "    # Start streaming from file\n",
    "    pipeline.start(config)\n",
    "\n",
    "    # Create opencv window to render image in\n",
    "    #cv2.namedWindow(\"Depth Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "    cv2.namedWindow(\"Color Stream\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    # Create colorizer object\n",
    "    colorizer = rs.colorizer()\n",
    "    \n",
    "    #People detection\n",
    "    # initialize the HOG descriptor/person detector\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    \n",
    "    offset=20\n",
    "    \n",
    "    #Prepare a Video Setups\n",
    "    #height, width, channels = color_image.shape\n",
    "    height = 360\n",
    "    width = 640\n",
    "    out = cv2.VideoWriter('out.avi',cv2.VideoWriter_fourcc(*'mp4v'), 30,(width,height))\n",
    "\n",
    "    # Streaming loop\n",
    "    while True:\n",
    "        # Get frameset of depth\n",
    "        frames = pipeline.wait_for_frames()\n",
    "\n",
    "        # Get depth frame\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        # Colorize depth frame to jet colormap\n",
    "        depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "        # Convert depth_frame to numpy array to render image in opencv\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.cvtColor(color_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        \n",
    "        #People detection\n",
    "        \n",
    "        # resizing for faster detection\n",
    "        color_image = cv2.resize(color_image, (width, height))\n",
    "        depth_color_image = cv2.resize(depth_color_image, (width, height))\n",
    "    \n",
    "        # using a greyscale picture, also for faster detection\n",
    "        gray = cv2.cvtColor(color_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # detect people in the image\n",
    "        # returns the bounding boxes for the detected objects\n",
    "        boxes, weights = hog.detectMultiScale(gray, hitThreshold=0.8, winStride=(8,8) )\n",
    "        \n",
    "        boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "        for (xA, yA, xB, yB) in boxes:\n",
    "            # display the detected boxes in the colour picture\n",
    "            cv2.rectangle(color_image, (xA+offset, yA+offset), (xB-offset, yB-offset), (255, 255, 255), 2)\n",
    "            roi_depth = (xA, yA, xB, yB)\n",
    "            mean_depth = np.mean(roi_depth)\n",
    "            distace =str(mean_depth)+' cm'\n",
    "            cv2.putText(color_image, distace, (xA+offset, yA+offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "    \n",
    "        # Render image in opencv window\n",
    "        #cv2.imshow(\"Depth Stream\", depth_color_image)\n",
    "        cv2.imshow(\"Color Stream\", color_image)\n",
    "        key = cv2.waitKey(1)\n",
    "        \n",
    "        \n",
    "        #Write to AVI file\n",
    "        out.write(color_image)\n",
    "\n",
    "        \n",
    "        # if pressed escape exit program\n",
    "        if key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting 2D pixel coordinate to 3D point coordinate using Intel Realsense SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = rs.pipeline()\n",
    "\n",
    "# Create a config object\n",
    "config = rs.config()\n",
    "\n",
    "# Tell config that we will use a recorded device from file to be used by the pipeline through playback.\n",
    "rs.config.enable_device_from_file(config, '../week8/001.bag')\n",
    "\n",
    "# Configure the pipeline to stream the depth stream\n",
    "# Change this parameters according to the recorded bag file resolution\n",
    "# config.enable_stream(rs.stream.depth, rs.format.z16, 30)\n",
    "\n",
    "# Start streaming\n",
    "profile = pipeline.start(config)\n",
    "\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "    \n",
    "frames = pipeline.wait_for_frames()\n",
    "\n",
    "# alin the depth and color frame\n",
    "aligned_frames = align.process(frames)\n",
    "aligned_frames = frames\n",
    "\n",
    "# Get depth and color frame\n",
    "depth_frame = frames.get_depth_frame()\n",
    "color_frame = frames.get_color_frame()\n",
    "\n",
    "#Get camera intrin\n",
    "camera_intrin = depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "print(camera_intrin)\n",
    "\n",
    "depth_image = np.asanyarray(depth_frame.get_data())\n",
    "color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "x = 100\n",
    "y = 100\n",
    "\n",
    "depth = depth_image[x, y]\n",
    "color = color_image[x, y]\n",
    "\n",
    "world_coordinate = rs.rs2_deproject_pixel_to_point(camera_intrin, [x, y], depth)\n",
    "print(world_coordinate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classwork(Teamwork): Plot the tracking results of mediapipe in the world coordinate\n",
    "\n",
    "In this work, we will use the rs2_deporject_pixel_to_point to project the 2d tracking results of mediapipe (such as a human skeleton, hands or facial expressions) to 3d point world coordinate. \n",
    "\n",
    "Input: tracking results of mediapipe (2d pixel)\n",
    "\n",
    "Output: tracking results of mediapipe (3d point world coordinate)\n",
    "\n",
    "![title](res_1116.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mocap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
